{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "lhcxlr2LUw0Z"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S91OAgfMSrgm",
        "outputId": "2b7e6441-3228-44af-a700-da5b3d4d27ac"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = \"/content/drive/MyDrive/makemore_course/\""
      ],
      "metadata": {
        "id": "hGbwB5sdUlRr"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NAMES = os.path.join(BASE_PATH, 'names.txt')"
      ],
      "metadata": {
        "id": "DwB5tKuFUrEe"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names = []\n",
        "for f in open(NAMES, \"r\"):\n",
        "  names.append(f[:-1])"
      ],
      "metadata": {
        "id": "DtodRcKQU2P0"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**E02**: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
      ],
      "metadata": {
        "id": "vhW1Tols7KsE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "vPGJ5lMR1u2f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import itertools\n",
        "import random\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alphabet = list(map(chr, range(97, 123)))\n",
        "alphabet[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnqRia8n1xdj",
        "outputId": "7f31958f-b8de-493f-babe-d062c31e9191"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a', 'b', 'c', 'd', 'e']"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "comb2 = list(map(lambda x : x[0]+x[1], itertools.product(alphabet,repeat = 2))) \n",
        "# we are missing the start end token \".\", let's add it.\n",
        "# we can have it either in front or in the back of any letter.\n",
        "s = list(map(lambda x: x[0]+x[1], zip([\".\" for _ in range(26)],alphabet)))\n",
        "e = list(map(lambda x: x[0]+x[1], zip(alphabet, [\".\" for _ in range(26)])))\n",
        "# add to our list of possible combinations:\n",
        "comb2 += s\n",
        "comb2 += e"
      ],
      "metadata": {
        "id": "88oRtXlY3jUZ"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# So what we have now is the possible first part of our trigram,\n",
        "# which consist of 2 chars\n",
        "# We need a way of encoding it.\n",
        "chars_to_idx = {chars:idx for idx, chars in enumerate(comb2)}\n",
        "# Also we want a mapping back:\n",
        "idx_to_chars = {idx:chars for chars, idx in chars_to_idx.items()}"
      ],
      "metadata": {
        "id": "uatbJPCi5mU-"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars_to_idx[\".e\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShP-c0SnF7cq",
        "outputId": "4f701777-f521-42c6-80d2-77204eb7574c"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "680"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Also we want a mapping back:\n",
        "idx_to_chars = {idx:chars for chars, idx in chars_to_idx.items()}\n",
        "# analog for the second part of the trigram.\n",
        "char_to_idx = {char:idx for idx, char in enumerate(['.'] + alphabet)}\n",
        "idx_to_char = {idx:char for char, idx in char_to_idx.items()}"
      ],
      "metadata": {
        "id": "BfNJhd9D5osm"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset_trigram(words):\n",
        "  x_s = []\n",
        "  y_s = []\n",
        "  for word in words:\n",
        "    for x,y in zip(map(lambda x: x[0]+x[1], \n",
        "                      (zip(\".\" + word, word[:]))),[x for x in word[1:] + \".\"]):\n",
        "      x_i = chars_to_idx[x]\n",
        "      y_i = char_to_idx[y]\n",
        "      x_s.append(x_i)\n",
        "      y_s.append(y_i)\n",
        "  return torch.tensor(x_s), torch.tensor(y_s)"
      ],
      "metadata": {
        "id": "TMMsvfJ6G_Ai"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset_bigram(words):\n",
        "  # create the training set of bigrams (x,y)\n",
        "  xs, ys = [], []\n",
        "\n",
        "  for w in words:\n",
        "    chs = ['.'] + list(w) + ['.']\n",
        "    for ch1, ch2 in zip(chs, chs[1:]):\n",
        "      ix1 = char_to_idx[ch1]\n",
        "      ix2 = char_to_idx[ch2]\n",
        "      xs.append(ix1)\n",
        "      ys.append(ix2)\n",
        "      \n",
        "  xs = torch.tensor(xs)\n",
        "  ys = torch.tensor(ys)\n",
        "  return xs, ys"
      ],
      "metadata": {
        "id": "yKwFa_LJ_lb2"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weight_trigram():\n",
        "  return torch.randn((728,27), requires_grad=True)"
      ],
      "metadata": {
        "id": "HpCGy0U6OHrf"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weight_bigram():\n",
        "  return torch.randn((27,27), requires_grad=True)"
      ],
      "metadata": {
        "id": "qkj0ZnvUAkak"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_bigram(xs):\n",
        "  xenc = F.one_hot(xs, num_classes=27).float()\n",
        "  return xenc"
      ],
      "metadata": {
        "id": "6CFCYD9oAn-n"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_trigram(xs):\n",
        "  xenc = F.one_hot(xs, num_classes=728).float()\n",
        "  return xenc"
      ],
      "metadata": {
        "id": "VhPM5qrpA2sQ"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "#------------bigram--------------------\n",
        "xsb, ysb = create_dataset_bigram(names)\n",
        "xencb = encode_bigram(xsb)\n",
        "xb_, xbtst, yb_, ybtst = train_test_split(xencb, ysb, test_size=0.1)\n",
        "xbtrn, xbdev, ybtrn, ybdev = train_test_split(xb_, yb_, test_size=0.1/0.9)\n",
        "#------------trigram--------------------\n",
        "xst, yst = create_dataset_trigram(names)\n",
        "xenct = encode_trigram(xst)\n",
        "xt_, xttst, yt_, yttst = train_test_split(xenct, yst, test_size=0.1)\n",
        "xttrn, xtdev, yttrn, ytdev = train_test_split(xt_, yt_, test_size=0.1/0.9)"
      ],
      "metadata": {
        "id": "X6rPLNCXBbsH"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps for training sumarized:"
      ],
      "metadata": {
        "id": "p8hV8zBMWT-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calc log counts:\n",
        "def calc_log_counts(xenc, W):\n",
        "  return xenc@W"
      ],
      "metadata": {
        "id": "yOFAgcgGV-5n"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calc counts:\n",
        "def calc_counts(logits):\n",
        "  return logits.exp()"
      ],
      "metadata": {
        "id": "h_Sd17pJWJjm"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calc probs:\n",
        "def calc_probs(counts):\n",
        "  return counts/counts.sum(1, keepdims=True)"
      ],
      "metadata": {
        "id": "wEs8soavWREt"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the loss:\n",
        "# In we are interested that the joined probs are close to 1\n",
        "# take the log of joints probs as log(a*b) = log(a)+log(b)\n",
        "# in each step we are interested in the log(prob) given the labels.\n",
        "# As log(1) = 0 our goal and bad loss is high positive number multiply by -1\n",
        "\n",
        "def calc_log_prob(probs, y):\n",
        "  n = len(y)\n",
        "  log_prob = -probs[torch.arange(n),y].log().mean()\n",
        "  return log_prob"
      ],
      "metadata": {
        "id": "Yax39MTDXSd5"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A step towards the right solution ;-):\n",
        "def step(xenc, ys, W, lr, reg):\n",
        "  # zero out the gradient:\n",
        "  W.grad = None\n",
        "  logits = calc_log_counts(xenc, W)\n",
        "  counts = calc_counts(logits)\n",
        "  probs = calc_probs(counts)\n",
        "  loss = calc_log_prob(probs, ys)\n",
        "  # add regularization!\n",
        "  loss += reg*(W**2).mean()\n",
        "  # accumalate gradients\n",
        "  loss.backward()\n",
        "  # update!\n",
        "  with torch.no_grad():\n",
        "    W -= lr * W.grad\n",
        "  return loss, W"
      ],
      "metadata": {
        "id": "8UkFBU0sWpUj"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Wb = init_weight_bigram()\n",
        "Wt = init_weight_trigram()"
      ],
      "metadata": {
        "id": "hK5YWn8fX2ry"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run a whole training loop:\n",
        "for k in range(300):\n",
        "  lossb, Wb = step(xbtrn, ybtrn, Wb, 50, 0.01)\n",
        "  losst, Wt = step(xttrn, yttrn, Wt, 500, 0.01)\n",
        "  if k%5 == 0:\n",
        "    print(f\"epoch = {k}\")\n",
        "    print(f\"current loss = {lossb.item()}\")\n",
        "    print(f\"current loss = {losst.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UurYDHdtX4E7",
        "outputId": "64375664-77cd-4c18-d9b2-4a40145ebe99"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch = 0\n",
            "current loss = 3.6050350666046143\n",
            "current loss = 3.819342613220215\n",
            "epoch = 5\n",
            "current loss = 2.8077731132507324\n",
            "current loss = 2.5973057746887207\n",
            "epoch = 10\n",
            "current loss = 2.668156147003174\n",
            "current loss = 2.4158191680908203\n",
            "epoch = 15\n",
            "current loss = 2.6057846546173096\n",
            "current loss = 2.326489210128784\n",
            "epoch = 20\n",
            "current loss = 2.571425676345825\n",
            "current loss = 2.2787113189697266\n",
            "epoch = 25\n",
            "current loss = 2.55021333694458\n",
            "current loss = 2.244295120239258\n",
            "epoch = 30\n",
            "current loss = 2.5359885692596436\n",
            "current loss = 2.2206623554229736\n",
            "epoch = 35\n",
            "current loss = 2.525876045227051\n",
            "current loss = 2.2119650840759277\n",
            "epoch = 40\n",
            "current loss = 2.518364667892456\n",
            "current loss = 2.215937376022339\n",
            "epoch = 45\n",
            "current loss = 2.512594699859619\n",
            "current loss = 2.186171770095825\n",
            "epoch = 50\n",
            "current loss = 2.508045196533203\n",
            "current loss = 2.193129777908325\n",
            "epoch = 55\n",
            "current loss = 2.504382371902466\n",
            "current loss = 2.166161060333252\n",
            "epoch = 60\n",
            "current loss = 2.5013837814331055\n",
            "current loss = 2.1584830284118652\n",
            "epoch = 65\n",
            "current loss = 2.4988951683044434\n",
            "current loss = 2.1737987995147705\n",
            "epoch = 70\n",
            "current loss = 2.49680495262146\n",
            "current loss = 2.1502809524536133\n",
            "epoch = 75\n",
            "current loss = 2.4950313568115234\n",
            "current loss = 2.1501073837280273\n",
            "epoch = 80\n",
            "current loss = 2.4935131072998047\n",
            "current loss = 2.162642002105713\n",
            "epoch = 85\n",
            "current loss = 2.492203950881958\n",
            "current loss = 2.1403419971466064\n",
            "epoch = 90\n",
            "current loss = 2.4910664558410645\n",
            "current loss = 2.145362138748169\n",
            "epoch = 95\n",
            "current loss = 2.49007248878479\n",
            "current loss = 2.1340579986572266\n",
            "epoch = 100\n",
            "current loss = 2.4891998767852783\n",
            "current loss = 2.1325950622558594\n",
            "epoch = 105\n",
            "current loss = 2.488429307937622\n",
            "current loss = 2.1401095390319824\n",
            "epoch = 110\n",
            "current loss = 2.487746238708496\n",
            "current loss = 2.1308770179748535\n",
            "epoch = 115\n",
            "current loss = 2.487138271331787\n",
            "current loss = 2.1327431201934814\n",
            "epoch = 120\n",
            "current loss = 2.486595392227173\n",
            "current loss = 2.124225616455078\n",
            "epoch = 125\n",
            "current loss = 2.4861083030700684\n",
            "current loss = 2.1246156692504883\n",
            "epoch = 130\n",
            "current loss = 2.485670804977417\n",
            "current loss = 2.145967483520508\n",
            "epoch = 135\n",
            "current loss = 2.485275983810425\n",
            "current loss = 2.122873306274414\n",
            "epoch = 140\n",
            "current loss = 2.4849185943603516\n",
            "current loss = 2.119077682495117\n",
            "epoch = 145\n",
            "current loss = 2.4845948219299316\n",
            "current loss = 2.144387722015381\n",
            "epoch = 150\n",
            "current loss = 2.484299898147583\n",
            "current loss = 2.1256000995635986\n",
            "epoch = 155\n",
            "current loss = 2.4840312004089355\n",
            "current loss = 2.135528802871704\n",
            "epoch = 160\n",
            "current loss = 2.4837849140167236\n",
            "current loss = 2.116590976715088\n",
            "epoch = 165\n",
            "current loss = 2.4835596084594727\n",
            "current loss = 2.1285345554351807\n",
            "epoch = 170\n",
            "current loss = 2.4833524227142334\n",
            "current loss = 2.1233861446380615\n",
            "epoch = 175\n",
            "current loss = 2.4831619262695312\n",
            "current loss = 2.113630533218384\n",
            "epoch = 180\n",
            "current loss = 2.4829862117767334\n",
            "current loss = 2.1133995056152344\n",
            "epoch = 185\n",
            "current loss = 2.482823133468628\n",
            "current loss = 2.1389667987823486\n",
            "epoch = 190\n",
            "current loss = 2.4826724529266357\n",
            "current loss = 2.1131362915039062\n",
            "epoch = 195\n",
            "current loss = 2.4825327396392822\n",
            "current loss = 2.1106576919555664\n",
            "epoch = 200\n",
            "current loss = 2.4824023246765137\n",
            "current loss = 2.1112892627716064\n",
            "epoch = 205\n",
            "current loss = 2.482281446456909\n",
            "current loss = 2.1244475841522217\n",
            "epoch = 210\n",
            "current loss = 2.482168436050415\n",
            "current loss = 2.116180419921875\n",
            "epoch = 215\n",
            "current loss = 2.482063055038452\n",
            "current loss = 2.1357243061065674\n",
            "epoch = 220\n",
            "current loss = 2.481964111328125\n",
            "current loss = 2.114788055419922\n",
            "epoch = 225\n",
            "current loss = 2.4818713665008545\n",
            "current loss = 2.1348226070404053\n",
            "epoch = 230\n",
            "current loss = 2.4817850589752197\n",
            "current loss = 2.1140224933624268\n",
            "epoch = 235\n",
            "current loss = 2.481703519821167\n",
            "current loss = 2.1335625648498535\n",
            "epoch = 240\n",
            "current loss = 2.481626510620117\n",
            "current loss = 2.116529703140259\n",
            "epoch = 245\n",
            "current loss = 2.4815542697906494\n",
            "current loss = 2.1077561378479004\n",
            "epoch = 250\n",
            "current loss = 2.4814865589141846\n",
            "current loss = 2.1054470539093018\n",
            "epoch = 255\n",
            "current loss = 2.4814226627349854\n",
            "current loss = 2.134370803833008\n",
            "epoch = 260\n",
            "current loss = 2.4813621044158936\n",
            "current loss = 2.1170477867126465\n",
            "epoch = 265\n",
            "current loss = 2.481304407119751\n",
            "current loss = 2.1116132736206055\n",
            "epoch = 270\n",
            "current loss = 2.481250762939453\n",
            "current loss = 2.1317498683929443\n",
            "epoch = 275\n",
            "current loss = 2.481199026107788\n",
            "current loss = 2.1055991649627686\n",
            "epoch = 280\n",
            "current loss = 2.4811506271362305\n",
            "current loss = 2.1036877632141113\n",
            "epoch = 285\n",
            "current loss = 2.4811043739318848\n",
            "current loss = 2.1245996952056885\n",
            "epoch = 290\n",
            "current loss = 2.4810609817504883\n",
            "current loss = 2.104435682296753\n",
            "epoch = 295\n",
            "current loss = 2.4810194969177246\n",
            "current loss = 2.1130425930023193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits = calc_log_counts(xbdev, Wb)\n",
        "counts = calc_counts(logits)\n",
        "probs = calc_probs(counts)\n",
        "loss = calc_log_prob(probs, ybdev)\n",
        "print(f\"Bigram loss on dev set   = {loss.item()}\")\n",
        "logits = calc_log_counts(xbtst, Wb)\n",
        "counts = calc_counts(logits)\n",
        "probs = calc_probs(counts)\n",
        "loss = calc_log_prob(probs, ybtst)\n",
        "print(f\"Bigram loss on test set  = {loss.item()}\")\n",
        "logits = calc_log_counts(xtdev, Wt)\n",
        "counts = calc_counts(logits)\n",
        "probs = calc_probs(counts)\n",
        "loss = calc_log_prob(probs, ytdev)\n",
        "print(f\"Trigram loss on dev set  = {loss.item()}\")\n",
        "logits = calc_log_counts(xttst, Wt)\n",
        "counts = calc_counts(logits)\n",
        "probs = calc_probs(counts)\n",
        "loss = calc_log_prob(probs, yttst)\n",
        "print(f\"Trigram loss on test set = {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAHoAwO-RcfP",
        "outputId": "f1128921-3e3b-4d45-8a46-ab5f5e3bf423"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram loss on dev set   = 2.468491315841675\n",
            "Bigram loss on test set  = 2.462188720703125\n",
            "Trigram loss on dev set  = 2.1004040241241455\n",
            "Trigram loss on test set = 2.119518756866455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**E03**: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?"
      ],
      "metadata": {
        "id": "gBKyS00DUuUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regs = torch.logspace(start=-7, end=-4, steps=20)\n",
        "regs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dE31fPbMUtDL",
        "outputId": "1ed0ad5a-7677-49b7-c103-42d5633687df"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0000e-07, 1.4384e-07, 2.0691e-07, 2.9764e-07, 4.2813e-07, 6.1585e-07,\n",
              "        8.8587e-07, 1.2743e-06, 1.8330e-06, 2.6367e-06, 3.7927e-06, 5.4556e-06,\n",
              "        7.8476e-06, 1.1288e-05, 1.6238e-05, 2.3357e-05, 3.3598e-05, 4.8329e-05,\n",
              "        6.9519e-05, 1.0000e-04])"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "for reg in regs:\n",
        "  Wb = init_weight_bigram()\n",
        "  Wt = init_weight_trigram()\n",
        "  # run a whole training loop:\n",
        "  for k in range(50):\n",
        "    lossb, Wb = step(xbdev, ybdev, Wb, 50, reg)\n",
        "    losst, Wt = step(xtdev, ytdev, Wt, 500, reg)\n",
        "    if k == 49:\n",
        "      print(\"-\"*60)\n",
        "      print(f\"reg = {reg}\")\n",
        "      print(f\"final loss bigram  = {lossb.item()}\")\n",
        "      print(f\"final loss trigram = {losst.item()}\")\n",
        "      print(\"-\"*20 + \" EVAL ON TRAIN DATA \" + \"-\"*20)\n",
        "      logits = calc_log_counts(xbtrn, Wb)\n",
        "      counts = calc_counts(logits)\n",
        "      probs = calc_probs(counts)\n",
        "      loss = calc_log_prob(probs, ybtrn)\n",
        "      print(f\"bigram loss on the train set = {loss}\")\n",
        "      logits = calc_log_counts(xttrn, Wt)\n",
        "      counts = calc_counts(logits)\n",
        "      probs = calc_probs(counts)\n",
        "      loss = calc_log_prob(probs, yttrn)\n",
        "      print(f\"trigram loss on the train set = {loss}\")\n",
        "end = time.time()\n",
        "eval_time1 = end-start"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvD9-ZlWVtNo",
        "outputId": "899f6cce-56cc-49d5-e954-ed56c02f77da"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------\n",
            "reg = 1.0000000116860974e-07\n",
            "final loss bigram  = 2.501140594482422\n",
            "final loss trigram = 2.056353807449341\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.510770082473755\n",
            "trigram loss on the train set = 2.2604286670684814\n",
            "------------------------------------------------------------\n",
            "reg = 1.4384498570052529e-07\n",
            "final loss bigram  = 2.4968645572662354\n",
            "final loss trigram = 2.068697214126587\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.5062930583953857\n",
            "trigram loss on the train set = 2.259241819381714\n",
            "------------------------------------------------------------\n",
            "reg = 2.069138105298407e-07\n",
            "final loss bigram  = 2.496697187423706\n",
            "final loss trigram = 2.059469223022461\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.50626277923584\n",
            "trigram loss on the train set = 2.268860101699829\n",
            "------------------------------------------------------------\n",
            "reg = 2.976351538563904e-07\n",
            "final loss bigram  = 2.4945671558380127\n",
            "final loss trigram = 2.0584652423858643\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.5025601387023926\n",
            "trigram loss on the train set = 2.261018753051758\n",
            "------------------------------------------------------------\n",
            "reg = 4.281332337541244e-07\n",
            "final loss bigram  = 2.4966745376586914\n",
            "final loss trigram = 2.060055732727051\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.506014823913574\n",
            "trigram loss on the train set = 2.277371644973755\n",
            "------------------------------------------------------------\n",
            "reg = 6.158481937745819e-07\n",
            "final loss bigram  = 2.495615243911743\n",
            "final loss trigram = 2.0596721172332764\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.503960371017456\n",
            "trigram loss on the train set = 2.2728161811828613\n",
            "------------------------------------------------------------\n",
            "reg = 8.858667683853128e-07\n",
            "final loss bigram  = 2.495293617248535\n",
            "final loss trigram = 2.0572965145111084\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.5056443214416504\n",
            "trigram loss on the train set = 2.2601735591888428\n",
            "------------------------------------------------------------\n",
            "reg = 1.274274950446852e-06\n",
            "final loss bigram  = 2.4907495975494385\n",
            "final loss trigram = 2.055621862411499\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.5001769065856934\n",
            "trigram loss on the train set = 2.260047674179077\n",
            "------------------------------------------------------------\n",
            "reg = 1.832980728977418e-06\n",
            "final loss bigram  = 2.4935436248779297\n",
            "final loss trigram = 2.0578854084014893\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.502950429916382\n",
            "trigram loss on the train set = 2.258787155151367\n",
            "------------------------------------------------------------\n",
            "reg = 2.636650833665044e-06\n",
            "final loss bigram  = 2.4977025985717773\n",
            "final loss trigram = 2.0593621730804443\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.507267951965332\n",
            "trigram loss on the train set = 2.2632083892822266\n",
            "------------------------------------------------------------\n",
            "reg = 3.7926902223262005e-06\n",
            "final loss bigram  = 2.490834951400757\n",
            "final loss trigram = 2.068539619445801\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.500180721282959\n",
            "trigram loss on the train set = 2.2586283683776855\n",
            "------------------------------------------------------------\n",
            "reg = 5.455594873637892e-06\n",
            "final loss bigram  = 2.4947104454040527\n",
            "final loss trigram = 2.0571134090423584\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.504298686981201\n",
            "trigram loss on the train set = 2.2607812881469727\n",
            "------------------------------------------------------------\n",
            "reg = 7.84759959060466e-06\n",
            "final loss bigram  = 2.492863178253174\n",
            "final loss trigram = 2.0577285289764404\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.5023696422576904\n",
            "trigram loss on the train set = 2.2627315521240234\n",
            "------------------------------------------------------------\n",
            "reg = 1.128837902797386e-05\n",
            "final loss bigram  = 2.496248483657837\n",
            "final loss trigram = 2.057457208633423\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.505295991897583\n",
            "trigram loss on the train set = 2.2613613605499268\n",
            "------------------------------------------------------------\n",
            "reg = 1.6237767340498976e-05\n",
            "final loss bigram  = 2.4975874423980713\n",
            "final loss trigram = 2.062593936920166\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.506756067276001\n",
            "trigram loss on the train set = 2.2724289894104004\n",
            "------------------------------------------------------------\n",
            "reg = 2.335721546842251e-05\n",
            "final loss bigram  = 2.4968924522399902\n",
            "final loss trigram = 2.0553665161132812\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.506443977355957\n",
            "trigram loss on the train set = 2.2576332092285156\n",
            "------------------------------------------------------------\n",
            "reg = 3.359818219905719e-05\n",
            "final loss bigram  = 2.4966063499450684\n",
            "final loss trigram = 2.069988965988159\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.5059497356414795\n",
            "trigram loss on the train set = 2.258334159851074\n",
            "------------------------------------------------------------\n",
            "reg = 4.8329300625482574e-05\n",
            "final loss bigram  = 2.4966163635253906\n",
            "final loss trigram = 2.0595641136169434\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.50534725189209\n",
            "trigram loss on the train set = 2.2607390880584717\n",
            "------------------------------------------------------------\n",
            "reg = 6.951927935006097e-05\n",
            "final loss bigram  = 2.494843006134033\n",
            "final loss trigram = 2.0570175647735596\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.5055580139160156\n",
            "trigram loss on the train set = 2.263230323791504\n",
            "------------------------------------------------------------\n",
            "reg = 9.999999747378752e-05\n",
            "final loss bigram  = 2.494274377822876\n",
            "final loss trigram = 2.0578818321228027\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.5047104358673096\n",
            "trigram loss on the train set = 2.2585744857788086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems that for the bigram model we can get better model with less data. \n"
      ],
      "metadata": {
        "id": "7j2d3cnOY_yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"BEST SETTINGS (POSSIBLE IMPROVEMENTS BY ADJUSTING LR AND TRAINING STEPS)\n",
        "------------------------------------------------------------\n",
        "reg = 4.281332337541244e-07\n",
        "final loss bigram  = 2.4863979816436768\n",
        "final loss trigram = 2.0799720287323\n",
        "-------------------- EVAL ON TRAIN DATA --------------------\n",
        "bigram loss on the train set = 2.500825881958008\n",
        "trigram loss on the train set = 2.2533843517303467\n",
        "------------------------------------------------------------\n",
        "------------------------------------------------------------\n",
        "reg = 1.6237767340498976e-05\n",
        "final loss bigram  = 2.4881796836853027\n",
        "final loss trigram = 2.0798087120056152\n",
        "-------------------- EVAL ON TRAIN DATA --------------------\n",
        "bigram loss on the train set = 2.504011631011963\n",
        "trigram loss on the train set = 2.2543632984161377\n",
        "------------------------------------------------------------\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "-eOnene6WjqN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "70d59958-3168-44f3-c8a1-2d35ccb7e59e"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'BEST SETTINGS (POSSIBLE IMPROVEMENTS BY ADJUSTING LR AND TRAINING STEPS)\\n------------------------------------------------------------\\nreg = 4.281332337541244e-07\\nfinal loss bigram  = 2.4863979816436768\\nfinal loss trigram = 2.0799720287323\\n-------------------- EVAL ON TRAIN DATA --------------------\\nbigram loss on the train set = 2.500825881958008\\ntrigram loss on the train set = 2.2533843517303467\\n------------------------------------------------------------\\n------------------------------------------------------------\\nreg = 1.6237767340498976e-05\\nfinal loss bigram  = 2.4881796836853027\\nfinal loss trigram = 2.0798087120056152\\n-------------------- EVAL ON TRAIN DATA --------------------\\nbigram loss on the train set = 2.504011631011963\\ntrigram loss on the train set = 2.2543632984161377\\n------------------------------------------------------------\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**E04**: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?"
      ],
      "metadata": {
        "id": "AjpcEI5AybqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A simple example:\n",
        "X = torch.tensor([[0,1],[1,0],[0,1]])\n",
        "X"
      ],
      "metadata": {
        "id": "r9AxoDsWakyb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b98938e0-90ca-4876-8ebb-f5a791535dfe"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 1],\n",
              "        [1, 0],\n",
              "        [0, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W = torch.arange(6).reshape(2,3)\n",
        "W"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kRxGj1XyphY",
        "outputId": "8aafd473-6db3-42c7-d042-d42c5cd02676"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 1, 2],\n",
              "        [3, 4, 5]])"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First column contains second column of W\n",
        "# Second column contains first column of W\n",
        "# Third like in first case\n",
        "X@W"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUXfXnjJy6iX",
        "outputId": "c8bcbb56-7157-4394-d435-b35b19d8ebaa"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[3, 4, 5],\n",
              "        [0, 1, 2],\n",
              "        [3, 4, 5]])"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Before OHE the the vector x was given by:\n",
        "X_s = torch.tensor([1,0,1])"
      ],
      "metadata": {
        "id": "Z4d153oLy9IU"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrcGkrH4vP4P",
        "outputId": "2affd92f-0965-40c6-dafe-a7a6db53c63f"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W[X_s]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqQc8_eAvpjU",
        "outputId": "1e67e7f9-218d-4229-fe41-7be9c7f000b8"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[3, 4, 5],\n",
              "        [0, 1, 2],\n",
              "        [3, 4, 5]])"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This archieves exactly the same!."
      ],
      "metadata": {
        "id": "xwxpf1dkvy8u"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calc log counts without encoding:\n",
        "def calc_log_counts_alt(x_s, W):\n",
        "  return W[x_s]"
      ],
      "metadata": {
        "id": "l4bropODwMlx"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A step towards the right solution ;-):\n",
        "def step_alt(x_s, ys, W, lr, reg):\n",
        "  # zero out the gradient:\n",
        "  W.grad = None\n",
        "  logits = calc_log_counts_alt(x_s, W)\n",
        "  counts = calc_counts(logits)\n",
        "  probs = calc_probs(counts)\n",
        "  loss = calc_log_prob(probs, ys)\n",
        "  # add regularization!\n",
        "  loss += reg*(W**2).mean()\n",
        "  # accumalate gradients\n",
        "  loss.backward()\n",
        "  # update!\n",
        "  with torch.no_grad():\n",
        "    W -= lr * W.grad\n",
        "  return loss, W"
      ],
      "metadata": {
        "id": "OEBOyKLPwgz9"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start = time.time()\n",
        "for _ in range(20):\n",
        "  step_alt(xst,yst, Wt, 500, 0.0001);\n",
        "end = time.time()\n",
        "\n",
        "print(f\"time it took = {end-start}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZSdPcGcwlfg",
        "outputId": "8d2cb834-c9ce-4b14-bf5a-0c7f79442e38"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time it took = 1.5573322772979736s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start = time.time()\n",
        "for _ in range(20):\n",
        "  step(xenct,yst, Wt, 500, 0.0001);\n",
        "end = time.time()\n",
        "\n",
        "print(f\"time it took = {end-start}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1S9ZksxxG9R",
        "outputId": "5bfe8ddd-21d9-45c7-c838-de56b210b59d"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time it took = 9.793309450149536s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The indexing method brings a significant upgrade in speed!"
      ],
      "metadata": {
        "id": "53kvN-zVxjw3"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create datasets for alternative method\n",
        "from sklearn.model_selection import train_test_split\n",
        "#------------bigram--------------------\n",
        "xsb, ysb = create_dataset_bigram(names)\n",
        "xb_, xbtst, yb_, ybtst = train_test_split(xsb, ysb, test_size=0.1)\n",
        "xbtrn, xbdev, ybtrn, ybdev = train_test_split(xb_, yb_, test_size=0.1/0.9)\n",
        "#------------trigram--------------------\n",
        "xst, yst = create_dataset_trigram(names)\n",
        "xt_, xttst, yt_, yttst = train_test_split(xst, yst, test_size=0.1)\n",
        "xttrn, xtdev, yttrn, ytdev = train_test_split(xt_, yt_, test_size=0.1/0.9)"
      ],
      "metadata": {
        "id": "MRbtaEzqyJZz"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "for reg in regs:\n",
        "  Wb = init_weight_bigram()\n",
        "  Wt = init_weight_trigram()\n",
        "  # run a whole training loop:\n",
        "  for k in range(50):\n",
        "    lossb, Wb = step_alt(xbdev, ybdev, Wb, 50, reg)\n",
        "    losst, Wt = step_alt(xtdev, ytdev, Wt, 500, reg)\n",
        "    if k == 49:\n",
        "      print(\"-\"*60)\n",
        "      print(f\"reg = {reg}\")\n",
        "      print(f\"final loss bigram  = {lossb.item()}\")\n",
        "      print(f\"final loss trigram = {losst.item()}\")\n",
        "      print(\"-\"*20 + \" EVAL ON TRAIN DATA \" + \"-\"*20)\n",
        "      logits = calc_log_counts_alt(xbtrn, Wb)\n",
        "      counts = calc_counts(logits)\n",
        "      probs = calc_probs(counts)\n",
        "      loss = calc_log_prob(probs, ybtrn)\n",
        "      print(f\"bigram loss on the train set = {loss}\")\n",
        "      logits = calc_log_counts_alt(xttrn, Wt)\n",
        "      counts = calc_counts(logits)\n",
        "      probs = calc_probs(counts)\n",
        "      loss = calc_log_prob(probs, yttrn)\n",
        "      print(f\"trigram loss on the train set = {loss}\")\n",
        "end = time.time()\n",
        "eval_time2 = end-start"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qi4pNYeHzwUO",
        "outputId": "68b0d1c6-d6ec-4d3e-c623-84f2f304431a"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------\n",
            "reg = 1.0000000116860974e-07\n",
            "final loss bigram  = 2.4864425659179688\n",
            "final loss trigram = 2.0778849124908447\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.500325918197632\n",
            "trigram loss on the train set = 2.256464958190918\n",
            "------------------------------------------------------------\n",
            "reg = 1.4384498570052529e-07\n",
            "final loss bigram  = 2.4858601093292236\n",
            "final loss trigram = 2.070258617401123\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.500339984893799\n",
            "trigram loss on the train set = 2.271763801574707\n",
            "------------------------------------------------------------\n",
            "reg = 2.069138105298407e-07\n",
            "final loss bigram  = 2.489915132522583\n",
            "final loss trigram = 2.067309856414795\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.503258228302002\n",
            "trigram loss on the train set = 2.2604968547821045\n",
            "------------------------------------------------------------\n",
            "reg = 2.976351538563904e-07\n",
            "final loss bigram  = 2.487185001373291\n",
            "final loss trigram = 2.0657362937927246\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.5020248889923096\n",
            "trigram loss on the train set = 2.253622055053711\n",
            "------------------------------------------------------------\n",
            "reg = 4.281332337541244e-07\n",
            "final loss bigram  = 2.486501932144165\n",
            "final loss trigram = 2.065389633178711\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.500248908996582\n",
            "trigram loss on the train set = 2.260256290435791\n",
            "------------------------------------------------------------\n",
            "reg = 6.158481937745819e-07\n",
            "final loss bigram  = 2.486441135406494\n",
            "final loss trigram = 2.0670065879821777\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.501129388809204\n",
            "trigram loss on the train set = 2.2559971809387207\n",
            "------------------------------------------------------------\n",
            "reg = 8.858667683853128e-07\n",
            "final loss bigram  = 2.4877755641937256\n",
            "final loss trigram = 2.068493127822876\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.502455711364746\n",
            "trigram loss on the train set = 2.2609879970550537\n",
            "------------------------------------------------------------\n",
            "reg = 1.274274950446852e-06\n",
            "final loss bigram  = 2.4869980812072754\n",
            "final loss trigram = 2.0779123306274414\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.501129627227783\n",
            "trigram loss on the train set = 2.2576310634613037\n",
            "------------------------------------------------------------\n",
            "reg = 1.832980728977418e-06\n",
            "final loss bigram  = 2.4883837699890137\n",
            "final loss trigram = 2.070194721221924\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.5031068325042725\n",
            "trigram loss on the train set = 2.2612345218658447\n",
            "------------------------------------------------------------\n",
            "reg = 2.636650833665044e-06\n",
            "final loss bigram  = 2.488666534423828\n",
            "final loss trigram = 2.0676307678222656\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.502689838409424\n",
            "trigram loss on the train set = 2.2580924034118652\n",
            "------------------------------------------------------------\n",
            "reg = 3.7926902223262005e-06\n",
            "final loss bigram  = 2.4852652549743652\n",
            "final loss trigram = 2.067056894302368\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.501004457473755\n",
            "trigram loss on the train set = 2.260101079940796\n",
            "------------------------------------------------------------\n",
            "reg = 5.455594873637892e-06\n",
            "final loss bigram  = 2.486353874206543\n",
            "final loss trigram = 2.079289436340332\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.5026259422302246\n",
            "trigram loss on the train set = 2.260673999786377\n",
            "------------------------------------------------------------\n",
            "reg = 7.84759959060466e-06\n",
            "final loss bigram  = 2.4894371032714844\n",
            "final loss trigram = 2.0679986476898193\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.50356388092041\n",
            "trigram loss on the train set = 2.2607014179229736\n",
            "------------------------------------------------------------\n",
            "reg = 1.128837902797386e-05\n",
            "final loss bigram  = 2.485363483428955\n",
            "final loss trigram = 2.0657339096069336\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.4991753101348877\n",
            "trigram loss on the train set = 2.2567784786224365\n",
            "------------------------------------------------------------\n",
            "reg = 1.6237767340498976e-05\n",
            "final loss bigram  = 2.48543381690979\n",
            "final loss trigram = 2.0674006938934326\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.4999921321868896\n",
            "trigram loss on the train set = 2.258474349975586\n",
            "------------------------------------------------------------\n",
            "reg = 2.335721546842251e-05\n",
            "final loss bigram  = 2.4849042892456055\n",
            "final loss trigram = 2.0696191787719727\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.4997506141662598\n",
            "trigram loss on the train set = 2.262139320373535\n",
            "------------------------------------------------------------\n",
            "reg = 3.359818219905719e-05\n",
            "final loss bigram  = 2.4874467849731445\n",
            "final loss trigram = 2.0702130794525146\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.5016112327575684\n",
            "trigram loss on the train set = 2.2629969120025635\n",
            "------------------------------------------------------------\n",
            "reg = 4.8329300625482574e-05\n",
            "final loss bigram  = 2.4858734607696533\n",
            "final loss trigram = 2.077535629272461\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.498807191848755\n",
            "trigram loss on the train set = 2.25453782081604\n",
            "------------------------------------------------------------\n",
            "reg = 6.951927935006097e-05\n",
            "final loss bigram  = 2.4899704456329346\n",
            "final loss trigram = 2.0799038410186768\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.5056519508361816\n",
            "trigram loss on the train set = 2.255976438522339\n",
            "------------------------------------------------------------\n",
            "reg = 9.999999747378752e-05\n",
            "final loss bigram  = 2.486112117767334\n",
            "final loss trigram = 2.066905975341797\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.5009853839874268\n",
            "trigram loss on the train set = 2.2604236602783203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"speedup for hyperparam: {100*(1-eval_time2/eval_time1):.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETcpLCwxz8c6",
        "outputId": "5de1d054-802a-4712-ef48-6a45db69a0d0"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speedup for hyperparam: 76.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**E05**: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?"
      ],
      "metadata": {
        "id": "1TJ4qtvX4KCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A step towards the right solution ;-):\n",
        "def step_alt(x_s, ys, W, lr, reg):\n",
        "  # zero out the gradient:\n",
        "  W.grad = None\n",
        "  logits = calc_log_counts_alt(x_s, W)\n",
        "  loss = F.cross_entropy(logits, ys)\n",
        "  # add regularization!\n",
        "  loss += reg*(W**2).mean()\n",
        "  # accumalate gradients\n",
        "  loss.backward()\n",
        "  # update!\n",
        "  with torch.no_grad():\n",
        "    W -= lr * W.grad\n",
        "  return loss, W"
      ],
      "metadata": {
        "id": "D10iPULQ2osk"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "for reg in regs:\n",
        "  Wb = init_weight_bigram()\n",
        "  Wt = init_weight_trigram()\n",
        "  # run a whole training loop:\n",
        "  for k in range(50):\n",
        "    lossb, Wb = step_alt(xbdev, ybdev, Wb, 50, reg)\n",
        "    losst, Wt = step_alt(xtdev, ytdev, Wt, 500, reg)\n",
        "    if k == 49:\n",
        "      print(\"-\"*60)\n",
        "      print(f\"reg = {reg}\")\n",
        "      print(f\"final loss bigram  = {lossb.item()}\")\n",
        "      print(f\"final loss trigram = {losst.item()}\")\n",
        "      print(\"-\"*20 + \" EVAL ON TRAIN DATA \" + \"-\"*20)\n",
        "      logits = calc_log_counts_alt(xbtrn, Wb)\n",
        "      counts = calc_counts(logits)\n",
        "      probs = calc_probs(counts)\n",
        "      loss = calc_log_prob(probs, ybtrn)\n",
        "      print(f\"bigram loss on the train set = {loss}\")\n",
        "      logits = calc_log_counts_alt(xttrn, Wt)\n",
        "      counts = calc_counts(logits)\n",
        "      probs = calc_probs(counts)\n",
        "      loss = calc_log_prob(probs, yttrn)\n",
        "      print(f\"trigram loss on the train set = {loss}\")\n",
        "end = time.time()\n",
        "eval_time3 = end-start"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v8rxAkA6DGR",
        "outputId": "8ca6e2f6-916d-43e1-8746-5542448b7f30"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------\n",
            "reg = 1.0000000116860974e-07\n",
            "final loss bigram  = 2.487001419067383\n",
            "final loss trigram = 2.0704164505004883\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.50141978263855\n",
            "trigram loss on the train set = 2.2719459533691406\n",
            "------------------------------------------------------------\n",
            "reg = 1.4384498570052529e-07\n",
            "final loss bigram  = 2.485849618911743\n",
            "final loss trigram = 2.0710623264312744\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.500710964202881\n",
            "trigram loss on the train set = 2.276031255722046\n",
            "------------------------------------------------------------\n",
            "reg = 2.069138105298407e-07\n",
            "final loss bigram  = 2.4877727031707764\n",
            "final loss trigram = 2.0697617530822754\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.5016205310821533\n",
            "trigram loss on the train set = 2.2576088905334473\n",
            "------------------------------------------------------------\n",
            "reg = 2.976351538563904e-07\n",
            "final loss bigram  = 2.4860479831695557\n",
            "final loss trigram = 2.068847894668579\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.5016367435455322\n",
            "trigram loss on the train set = 2.2603743076324463\n",
            "------------------------------------------------------------\n",
            "reg = 4.281332337541244e-07\n",
            "final loss bigram  = 2.4880220890045166\n",
            "final loss trigram = 2.0665061473846436\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.5045228004455566\n",
            "trigram loss on the train set = 2.260148048400879\n",
            "------------------------------------------------------------\n",
            "reg = 6.158481937745819e-07\n",
            "final loss bigram  = 2.4888200759887695\n",
            "final loss trigram = 2.0683295726776123\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.5029523372650146\n",
            "trigram loss on the train set = 2.259042263031006\n",
            "------------------------------------------------------------\n",
            "reg = 8.858667683853128e-07\n",
            "final loss bigram  = 2.488049030303955\n",
            "final loss trigram = 2.0685648918151855\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.5022239685058594\n",
            "trigram loss on the train set = 2.2586166858673096\n",
            "------------------------------------------------------------\n",
            "reg = 1.274274950446852e-06\n",
            "final loss bigram  = 2.490433931350708\n",
            "final loss trigram = 2.0763821601867676\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.505185842514038\n",
            "trigram loss on the train set = 2.2550554275512695\n",
            "------------------------------------------------------------\n",
            "reg = 1.832980728977418e-06\n",
            "final loss bigram  = 2.4865405559539795\n",
            "final loss trigram = 2.070913076400757\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.5017497539520264\n",
            "trigram loss on the train set = 2.2743842601776123\n",
            "------------------------------------------------------------\n",
            "reg = 2.636650833665044e-06\n",
            "final loss bigram  = 2.483891487121582\n",
            "final loss trigram = 2.0714187622070312\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.499518871307373\n",
            "trigram loss on the train set = 2.2700462341308594\n",
            "------------------------------------------------------------\n",
            "reg = 3.7926902223262005e-06\n",
            "final loss bigram  = 2.485034942626953\n",
            "final loss trigram = 2.069262742996216\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.500358819961548\n",
            "trigram loss on the train set = 2.2594988346099854\n",
            "------------------------------------------------------------\n",
            "reg = 5.455594873637892e-06\n",
            "final loss bigram  = 2.487980365753174\n",
            "final loss trigram = 2.068164825439453\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.5018250942230225\n",
            "trigram loss on the train set = 2.260160207748413\n",
            "------------------------------------------------------------\n",
            "reg = 7.84759959060466e-06\n",
            "final loss bigram  = 2.488630771636963\n",
            "final loss trigram = 2.068523406982422\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.5033910274505615\n",
            "trigram loss on the train set = 2.26202392578125\n",
            "------------------------------------------------------------\n",
            "reg = 1.128837902797386e-05\n",
            "final loss bigram  = 2.488084316253662\n",
            "final loss trigram = 2.0691967010498047\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.5023584365844727\n",
            "trigram loss on the train set = 2.2598652839660645\n",
            "------------------------------------------------------------\n",
            "reg = 1.6237767340498976e-05\n",
            "final loss bigram  = 2.4879636764526367\n",
            "final loss trigram = 2.0668327808380127\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.5011136531829834\n",
            "trigram loss on the train set = 2.2598354816436768\n",
            "------------------------------------------------------------\n",
            "reg = 2.335721546842251e-05\n",
            "final loss bigram  = 2.486107349395752\n",
            "final loss trigram = 2.0656049251556396\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.5014455318450928\n",
            "trigram loss on the train set = 2.2567031383514404\n",
            "------------------------------------------------------------\n",
            "reg = 3.359818219905719e-05\n",
            "final loss bigram  = 2.48820161819458\n",
            "final loss trigram = 2.0709331035614014\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.50130033493042\n",
            "trigram loss on the train set = 2.2747979164123535\n",
            "------------------------------------------------------------\n",
            "reg = 4.8329300625482574e-05\n",
            "final loss bigram  = 2.4857614040374756\n",
            "final loss trigram = 2.068187713623047\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.5000171661376953\n",
            "trigram loss on the train set = 2.258889675140381\n",
            "------------------------------------------------------------\n",
            "reg = 6.951927935006097e-05\n",
            "final loss bigram  = 2.4865972995758057\n",
            "final loss trigram = 2.06756329536438\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.502134323120117\n",
            "trigram loss on the train set = 2.2605931758880615\n",
            "------------------------------------------------------------\n",
            "reg = 9.999999747378752e-05\n",
            "final loss bigram  = 2.4863851070404053\n",
            "final loss trigram = 2.069267749786377\n",
            "-------------------- EVAL ON TRAIN DATA --------------------\n",
            "bigram loss on the train set = 2.501519203186035\n",
            "trigram loss on the train set = 2.2596707344055176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_time3/eval_time2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lt6fQT2v6L6u",
        "outputId": "44ab9ac7-296d-4213-8657-816033fdd6d7"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9705164269225395"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get a speedup by 3 percent. We (most of the times) prefer pytorch implementations as they are optimized for highest efficency and keep the code clean.\n",
        "\n",
        "A trade off is that it's less clear how to interprete the intermediate steps as they are not visible to us."
      ],
      "metadata": {
        "id": "UbsbWHD76ZEG"
      }
    }
  ]
}